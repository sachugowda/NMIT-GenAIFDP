
---

# Tutorial: Running Large Language Models at Home with Petals, BitTorrent-Style

Petals is an innovative platform that allows you to run large language models collaboratively on distributed hardware, using only a small part of the model on your local device. This approach enables you to access powerful models, like Llama 3.1 (up to 405B parameters) and BLOOM (176B parameters), using consumer-grade hardware, such as a typical GPU or Google Colab.

### Key Benefits of Petals
- **Collaborative Model Inference**: Load part of the model locally and join a network where others serve the remaining parts.
- **Affordable Access to Large Models**: Run massive models without the need for specialized infrastructure.
- **Flexible and Customizable**: Fine-tune and control various aspects of the model, from sampling methods to seeing hidden states.

## Table of Contents
1. [What is Petals?](#what-is-petals)
2. [Setting Up Petals](#setting-up-petals)
3. [Running a Large Language Model](#running-a-large-language-model)
4. [Fine-Tuning the Model for Your Tasks](#fine-tuning-the-model-for-your-tasks)
5. [Example Code](#example-code)
6. [Using Petals with Google Colab](#using-petals-with-google-colab)
7. [FAQs and Troubleshooting](#faqs-and-troubleshooting)

---

## What is Petals?

Petals is a platform for running large language models on distributed hardware. It uses a **BitTorrent-style approach** for model inference, where each user loads a part of the model and connects to a network of other users serving other parts. This collaborative framework allows users to access models that would otherwise be too large to run locally, sharing resources and capabilities.

With Petals, you can:
- Generate text using powerful models like **Llama 3.1**, **Mixtral**, **Falcon**, and **BLOOM**.
- Fine-tune models on your tasks using a consumer-grade GPU.
- Access up to 405B parameter models, thanks to the distributed model-sharing approach.

---

## Setting Up Petals

Before starting, make sure you have **Python 3.8 or later** and **PyTorch** installed. Petals also works well with **ðŸ¤— Transformers**, so having the `transformers` library is recommended.

### Step 1: Install Petals

Install Petals and the required libraries:

```bash
pip install petals transformers torch
```

### Step 2: Choose a Model

Petals supports a range of large language models. Here are some options you can choose from:
- **Llama 3.1** (up to 405B parameters)
- **Mixtral** (8 x 22B parameters)
- **Falcon** (40B+ parameters)
- **BLOOM** (176B parameters)

Refer to the Petals website for more details on the supported models and their specific capabilities.

---

## Running a Large Language Model

Petals allows you to load part of the model locally and access the rest through the distributed network. You can customize the modelâ€™s behavior, fine-tune it, or observe hidden states.

### Step 1: Initialize the Model

Letâ€™s initialize Llama 3.1 with 70B parameters as an example:

```python
from petals import AutoDistributedModelForCausalLM
from transformers import AutoTokenizer

# Load the tokenizer
tokenizer = AutoTokenizer.from_pretrained("petals-team/Llama-3.1-70B")

# Initialize the model with Petals
model = AutoDistributedModelForCausalLM.from_pretrained("petals-team/Llama-3.1-70B")
```

This code loads part of the Llama model and connects you to the Petals network to access the rest of the modelâ€™s layers.

### Step 2: Generate Text with the Model

You can now use the model to generate text:

```python
# Define your input prompt
input_prompt = "Explain the concept of quantum mechanics in simple terms."

# Tokenize the input
input_ids = tokenizer(input_prompt, return_tensors="pt").input_ids

# Generate output tokens
outputs = model.generate(input_ids, max_new_tokens=100)

# Decode and print the result
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

This code generates a continuation of the input prompt using the distributed Llama model. You should see a response generated by the large language model, powered by both your local machine and the Petals network.

---

## Fine-Tuning the Model for Your Tasks

Petals also allows you to fine-tune large models on your specific tasks, which can be useful if you want the model to adapt to a specialized domain or style.

### Example of Fine-Tuning

```python
from transformers import Trainer, TrainingArguments

# Prepare your dataset and define training arguments
training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=1,
    per_device_train_batch_size=2,
    save_steps=10,
    save_total_limit=2,
)

# Set up Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=your_train_dataset,  # Replace with your dataset
)

# Start fine-tuning
trainer.train()
```

In this example, replace `your_train_dataset` with your own dataset. The Petals platform allows you to adapt a small part of the model while benefiting from the full power of the distributed model for text generation.

---

## Example Code

Hereâ€™s the full code to set up and generate text with Petals using Llama 3.1 on a local or Colab environment.

```python
from petals import AutoDistributedModelForCausalLM
from transformers import AutoTokenizer

# Load the tokenizer and model
tokenizer = AutoTokenizer.from_pretrained("petals-team/Llama-3.1-70B")
model = AutoDistributedModelForCausalLM.from_pretrained("petals-team/Llama-3.1-70B")

# Define an input prompt
input_prompt = "What are the benefits of federated learning?"

# Tokenize and generate
input_ids = tokenizer(input_prompt, return_tensors="pt").input_ids
outputs = model.generate(input_ids, max_new_tokens=50)

# Decode and print the result
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

This code connects to the Petals network, generating text based on the Llama 3.1 model. You can modify the prompt to explore different topics and see how the model responds.

---

## Using Petals with Google Colab

If you donâ€™t have a GPU on your local machine, you can use Google Colab to run Petals.

1. **Open Google Colab**: Visit [Google Colab](https://colab.research.google.com/drive/1uCphNY7gfAUkdDrTx21dZZwCOUDCMPw8?usp=sharing#scrollTo=pBC52TF3LVY1) and create a new notebook.
2. **Enable GPU**: Go to **Runtime** > **Change runtime type** and select **GPU** as the hardware accelerator.
3. **Install Petals**: In a Colab cell, run:
   ```python
   !pip install petals transformers torch
   ```
4. **Run the Example Code**: Copy the example code above into Colab to load the model and generate text.

Using Petals on Colab allows you to access high-performance models without needing a local GPU.

---

## FAQs and Troubleshooting

### 1. Why is the generation speed slow?
Petals uses a distributed network of nodes, and response times may vary based on network availability. Single-batch inference runs at around 6 tokens/second on Llama 2 (70B) and up to 4 tokens/second on Falcon (180B), which is suitable for interactive applications but not for high-speed batch processing.

### 2. How is data shared in Petals?
Petals shares only model weights and does not transmit input data to other nodes, ensuring privacy and security.

### 3. Can I use Petals for fine-tuning?
Yes, Petals allows fine-tuning on specific tasks. You can adapt a portion of the model to better fit your domain while using the distributed model for inference.

### 4. Is Petals free to use?
Petals is a collaborative network, so you can access it as long as you contribute some computational resources. Itâ€™s designed to be cost-effective by utilizing shared hardware across users.

---

## Summary

In this tutorial, we explored how to use **Petals** to run large language models like Llama, BLOOM, and Falcon on distributed hardware, BitTorrent-style. By loading part of the model locally and accessing the rest via a collaborative network, Petals enables access to powerful models without needing specialized hardware. With this approach, you can:

- Run large models like Llama 3.1 and BLOOM on consumer-grade devices.
- Fine-tune models on specific tasks.
- Benefit from a flexible, customizable setup with the power of distributed machine learning.

Petals provides a new way to interact with large models, offering an affordable, decentralized, and scalable approach to leveraging advanced AI capabilities.
